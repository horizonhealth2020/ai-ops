# ──────────────────────────────────────────────────────────────────
# AI Ops Backend — Environment Variables
# Copy this file to .env and fill in your values.
# ──────────────────────────────────────────────────────────────────

# Server
PORT=3000
NODE_ENV=development

# PostgreSQL (provided by Railway PostgreSQL addon)
DATABASE_URL=postgresql://user:password@host:5432/dbname

# Vapi — set this as your custom LLM credential in the Vapi dashboard
VAPI_SECRET=your_vapi_secret_here

# ──────────────────────────────────────────────────────────────────
# LLM Provider — pick ONE configuration block below
# ──────────────────────────────────────────────────────────────────

# Option A: OpenAI
LLM_PROVIDER=openai
LLM_API_KEY=sk-...
LLM_MODEL=gpt-4o
# LLM_BASE_URL= (leave blank for default)

# Option B: Groq (fast open-source inference)
# LLM_PROVIDER=groq
# LLM_API_KEY=gsk_...
# LLM_MODEL=llama-3.3-70b-versatile

# Option C: Together AI
# LLM_PROVIDER=together
# LLM_API_KEY=your_together_key
# LLM_MODEL=meta-llama/Llama-3-70b-chat-hf

# Option D: Mistral AI
# LLM_PROVIDER=mistral
# LLM_API_KEY=your_mistral_key
# LLM_MODEL=mistral-large-latest

# Option E: Local Ollama (no API key needed)
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.2
# LLM_BASE_URL=http://localhost:11434/v1

# Option F: Anthropic
# LLM_PROVIDER=anthropic
# LLM_API_KEY=sk-ant-...
# LLM_MODEL=claude-opus-4-6

# Option G: Any OpenAI-compatible endpoint
# LLM_PROVIDER=custom
# LLM_API_KEY=your_api_key
# LLM_MODEL=your-model-name
# LLM_BASE_URL=https://your-api-endpoint/v1

# ──────────────────────────────────────────────────────────────────

# Stripe (for payment initiation)
STRIPE_SECRET_KEY=sk_test_...
